{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "noticed-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alternative-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder block based on the original paper\n",
    "def define_encoder_block(layer, filtersNo, batchnorm=True):\n",
    "    \n",
    "    # init weights from a Gaussian distribution with mean 0 and standard deviation 0.02\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    # in the original paper, all convolution kernels are (4,4), with stride 2. Stride for decoder means downsampling.\n",
    "    x = Conv2D(filtersNo, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer)\n",
    "    \n",
    "    # Conditional batch normalization (important for the first layer)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x, training=True)\n",
    "        \n",
    "    # All ReLUs in the encoder are leaky!\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "previous-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the decoder block based on the original paper\n",
    "def decoder_block(layer, skip, filtersNo, dropout=True, batch=True):\n",
    "    \n",
    "    # init weights from a Gaussian distribution with mean 0 and standard deviation 0.02\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    # in the original paper, all convolution kernels are (4,4), with stride 2. Stride for decoder means upsampling.\n",
    "    x = Conv2DTranspose(filtersNo, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer)\n",
    "\n",
    "    # All layers in the original paper have batch normalization, although we set an if statement just to play with the model\n",
    "    if batch:\n",
    "        x = BatchNormalization()(x, training=True)\n",
    "        \n",
    "    # Some decoder layers don't have dropout\n",
    "    if dropout:\n",
    "        x = Dropout(0.5)(x, training=True)\n",
    "        \n",
    "    # Merge with skip connection\n",
    "    x = Concatenate()([x, skip])\n",
    "    \n",
    "    # All ReLUs in the decoder are not leaky!\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gothic-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator based on encoder/decoder\n",
    "def define_generator():\n",
    "    \n",
    "    # init weights from a Gaussian distribution with mean 0 and standard deviation 0.02\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    # image input\n",
    "    inputImage = Input(shape=(128,128,1))\n",
    "    \n",
    "    ###### Encoder\n",
    "    \n",
    "    # C64, input (128,128,1), output (64,64,64)\n",
    "    encoderLayer1 = define_encoder_block(inputImage, 64, batchnorm=False)\n",
    "    \n",
    "    #C128, input (64,64,64), output (32,32,128)\n",
    "    encoderLayer2 = define_encoder_block(encoderLayer1, 128)\n",
    "    \n",
    "    #C256, input (32,32,128), output (16,16,256)\n",
    "    encoderLayer3 = define_encoder_block(encoderLayer2, 256)\n",
    "    \n",
    "    #C512, input (16,16,256), output (8,8,512)\n",
    "    encoderLayer4 = define_encoder_block(encoderLayer3, 512)\n",
    "    \n",
    "    #C512, input (8,8,512), output (4,4,512)\n",
    "    encoderLayer5 = define_encoder_block(encoderLayer4, 512)\n",
    "    \n",
    "    #C512, input (4,4,512), output (2,2,512)\n",
    "    encoderLayer6 = define_encoder_block(encoderLayer5, 512)\n",
    "    \n",
    "    ###### Bottleneck layer, will have an input of (2,2,512) and an output of (1,1,512)\n",
    "    bottleneck = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(encoderLayer6)\n",
    "    bottleneck = Activation('relu')(bottleneck)\n",
    "    \n",
    "    ###### Decoder, with skip connection\n",
    "    \n",
    "    #CD512\n",
    "    decoderLayer1 = decoder_block(bottleneck, encoderLayer6, 512)\n",
    "    \n",
    "    #CD512\n",
    "    decoderLayer2 = decoder_block(decoderLayer1, encoderLayer5, 512)\n",
    "    \n",
    "    #C512\n",
    "    decoderLayer3 = decoder_block(decoderLayer2, encoderLayer4, 512, dropout=False)\n",
    "    \n",
    "    #C256\n",
    "    decoderLayer4 = decoder_block(decoderLayer3, encoderLayer3, 256, dropout=False)\n",
    "    \n",
    "    #C128\n",
    "    decoderLayer5 = decoder_block(decoderLayer4, encoderLayer2, 128, dropout=False)\n",
    "    \n",
    "    #C64\n",
    "    decoderLayer6 = decoder_block(decoderLayer5, encoderLayer1, 64, dropout=False)\n",
    "    \n",
    "    # Output with tanh function, as mentioned in the original paper. Output will be (128x128x3)\n",
    "    g = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(decoderLayer6)\n",
    "    outputImage = Activation('tanh')(g)\n",
    "    \n",
    "    # Define model\n",
    "    model = Model(inputImage, outputImage)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "printable-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 70x70 discriminator as in the original paper\n",
    "def define_discriminator():\n",
    "    \n",
    "    # init weights from a Gaussian distribution with mean 0 and standard deviation 0.02\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    # source image input\n",
    "    source = Input(shape=(128,128,1))\n",
    "    \n",
    "    # target image input\n",
    "    target = Input(shape=(128,128,3))\n",
    "    \n",
    "    # concatenate images channel-wise\n",
    "    merged = Concatenate()([source, target])\n",
    "    \n",
    "    # C64\n",
    "    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    # C128\n",
    "    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    # C256\n",
    "    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    # C512\n",
    "    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    # patch output\n",
    "    d = Conv2D(1, (8,8), strides=(8,8), padding='same', kernel_initializer=init)(d)\n",
    "    patch_out = Activation('sigmoid')(d)\n",
    "    patch_out = Flatten()(patch_out)\n",
    "    \n",
    "    # define model\n",
    "    model = Model([source, target], patch_out)\n",
    "    \n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "loving-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pix2Pix GAN\n",
    "def pix2pix(generator, discriminator):\n",
    "    \n",
    "    # make weights in the discriminator not trainable\n",
    "    for layer in discriminator.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "            \n",
    "    # define the source image\n",
    "    source = Input(shape=(128,128,1))\n",
    "    \n",
    "    # connect the source image to the generator input\n",
    "    genOut = generator(source)\n",
    "    \n",
    "    # connect the source input and generator output to the discriminator input\n",
    "    disOut = discriminator([source, genOut])\n",
    "    \n",
    "    # src image as input, generated image and classification output\n",
    "    model = Model(source, [disOut, genOut])\n",
    "    \n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "previous-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFromDataset(trainX, trainY, samples):\n",
    "    \n",
    "    # Choose random images from both input and output\n",
    "    no = randint(0, trainX.shape[0], samples)\n",
    "    gx, gy = trainX[no], trainY[no]\n",
    "    \n",
    "    # Set y-labels to 1, as these images are from dataset\n",
    "    y = ones((samples, 1))\n",
    "    return [gx, gy], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fluid-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFromGenerator(generator, samples):\n",
    "    # Generate fake instance\n",
    "    x = generator.predict(samples)\n",
    "    \n",
    "    # Labels will be zero because they come from the generator\n",
    "    y = zeros((len(x), 1))\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "awful-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_performance(step, g_model, trainX, trainY, n_samples=5):\n",
    "    # select a sample of input images\n",
    "    [X_realA, X_realB], _ = generateFromDataset(trainX, trainY, n_samples)\n",
    "    \n",
    "    # Generate fake samples\n",
    "    X_fakeB, _ = generateFromGenerator(g_model, X_realA)\n",
    "    \n",
    "    # Plot black and white images\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(3, n_samples, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_realA[i])\n",
    "        \n",
    "    # Plot generated images\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(3, n_samples, 1 + n_samples + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_fakeB[i])\n",
    "        \n",
    "    # Plot expected output images\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_realB[i])\n",
    "        \n",
    "    # Save plot to file\n",
    "    filename1 = 'images/plot_%06d.png' % (step+1)\n",
    "    pyplot.savefig(filename1)\n",
    "    pyplot.close()\n",
    "    \n",
    "    # Save generator model\n",
    "    filename2 = 'models/model_%06d.h5' % (step+1)\n",
    "    g_model.save(filename2)\n",
    "    print('>Saved: %s and %s' % (filename1, filename2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "musical-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(discriminator, generator, gan, epochs=100000, samplesPerEpoch=250):\n",
    "    \n",
    "    # Load the data\n",
    "    bwData = load(\"Flickr8kblackandwhite1dim.npy\")\n",
    "    colorData = load(\"flickr8k_shuffled.npy\")\n",
    "    y = numpy.ones((8091,1))\n",
    "    \n",
    "    BWSplit = numpy.array_split(bwData, 2)\n",
    "    colorSplit = numpy.array_split(colorData, 2)\n",
    "    glossMin = 999\n",
    "    # manually enumerate epochs\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Generate real samples\n",
    "        [realX, realY], realLabel = generateFromDataset(BWSplit[0], colorSplit[0], samplesPerEpoch)\n",
    "        \n",
    "        # Generate fake samples\n",
    "        fakeY, fakeLabel = generateFromGenerator(generator, realX)\n",
    "        \n",
    "        # Update discriminator on real samples\n",
    "        realLoss = discriminator.train_on_batch([realX, realY], realLabel)\n",
    "        \n",
    "        # Update discriminator on fake samples\n",
    "        fakeLoss = discriminator.train_on_batch([realX, fakeY], fakeLabel)\n",
    "        \n",
    "        # Update generator\n",
    "        generatorLoss, _, _ = gan.train_on_batch(realX, [realLabel, realY])\n",
    "        \n",
    "        # summarize performance\n",
    "        print('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, realLoss, fakeLoss, generatorLoss))\n",
    "        if glossMin > generatorLoss:\n",
    "            summarize_performance(i, generator, BWSplit[0], colorSplit[0])\n",
    "            glossMin = generatorLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "boxed-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = define_discriminator()\n",
    "g = define_generator()\n",
    "p2p = pix2pix(g,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "coated-russian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, d1[0.513] d2[4.010] g[47.479]\n",
      ">2, d1[0.004] d2[4.568] g[45.026]\n",
      ">3, d1[0.547] d2[3.309] g[44.921]\n",
      ">4, d1[1.146] d2[2.595] g[46.058]\n",
      ">5, d1[1.104] d2[1.896] g[43.996]\n",
      ">6, d1[0.349] d2[0.728] g[42.321]\n",
      ">7, d1[0.455] d2[1.725] g[42.744]\n",
      ">8, d1[1.303] d2[0.734] g[38.042]\n",
      ">9, d1[0.073] d2[1.198] g[39.702]\n",
      ">10, d1[1.196] d2[0.673] g[34.039]\n",
      ">11, d1[0.010] d2[0.859] g[38.038]\n",
      ">12, d1[2.350] d2[2.229] g[33.224]\n",
      ">13, d1[0.023] d2[0.108] g[31.678]\n",
      ">14, d1[0.534] d2[2.712] g[30.911]\n",
      ">15, d1[1.975] d2[1.933] g[27.549]\n",
      ">16, d1[0.134] d2[0.204] g[26.350]\n",
      ">17, d1[0.761] d2[3.216] g[26.516]\n",
      ">18, d1[1.507] d2[1.207] g[27.279]\n",
      ">19, d1[1.584] d2[1.044] g[23.241]\n",
      ">20, d1[0.413] d2[3.478] g[27.134]\n",
      ">21, d1[2.734] d2[0.447] g[20.443]\n",
      ">22, d1[0.789] d2[1.378] g[20.012]\n",
      ">23, d1[1.028] d2[1.585] g[18.451]\n",
      ">24, d1[1.063] d2[2.177] g[22.369]\n",
      ">25, d1[3.802] d2[1.177] g[16.313]\n",
      ">26, d1[1.464] d2[2.126] g[18.183]\n",
      ">27, d1[2.082] d2[0.492] g[12.982]\n",
      ">28, d1[0.454] d2[0.907] g[16.332]\n",
      ">29, d1[0.874] d2[1.632] g[15.193]\n",
      ">30, d1[1.220] d2[0.930] g[14.463]\n",
      ">31, d1[0.929] d2[1.250] g[12.715]\n",
      ">32, d1[0.845] d2[0.578] g[13.364]\n",
      ">33, d1[1.017] d2[0.064] g[14.415]\n",
      ">34, d1[0.520] d2[1.750] g[12.304]\n",
      ">35, d1[0.441] d2[1.335] g[12.005]\n",
      ">36, d1[0.115] d2[0.070] g[12.997]\n",
      ">37, d1[0.724] d2[0.088] g[13.951]\n",
      ">38, d1[0.231] d2[1.716] g[13.224]\n",
      ">39, d1[1.688] d2[0.536] g[16.569]\n",
      ">40, d1[0.034] d2[0.110] g[15.303]\n",
      ">41, d1[2.399] d2[0.078] g[12.373]\n",
      ">42, d1[0.135] d2[0.179] g[13.580]\n",
      ">43, d1[0.160] d2[0.567] g[12.922]\n",
      ">44, d1[0.460] d2[2.755] g[12.122]\n",
      ">45, d1[1.550] d2[1.123] g[12.719]\n",
      ">46, d1[1.426] d2[1.179] g[12.160]\n",
      ">47, d1[0.727] d2[0.371] g[11.152]\n",
      ">48, d1[0.227] d2[0.291] g[11.170]\n",
      ">49, d1[0.124] d2[0.525] g[12.091]\n",
      ">50, d1[0.435] d2[0.010] g[10.597]\n",
      ">51, d1[0.174] d2[1.090] g[10.883]\n",
      ">52, d1[0.306] d2[0.011] g[11.016]\n",
      ">53, d1[0.205] d2[0.011] g[10.631]\n",
      ">54, d1[0.025] d2[0.110] g[10.524]\n",
      ">55, d1[0.014] d2[0.023] g[9.893]\n",
      ">56, d1[0.013] d2[0.014] g[9.876]\n",
      ">57, d1[0.011] d2[0.011] g[10.119]\n",
      ">58, d1[0.008] d2[0.033] g[9.189]\n",
      ">59, d1[0.010] d2[0.002] g[9.698]\n",
      ">60, d1[0.010] d2[0.010] g[9.552]\n",
      ">61, d1[0.009] d2[0.055] g[9.700]\n",
      ">62, d1[0.014] d2[0.034] g[9.601]\n",
      ">63, d1[0.014] d2[0.434] g[10.363]\n",
      ">64, d1[0.102] d2[0.003] g[13.607]\n",
      ">65, d1[0.256] d2[0.000] g[17.044]\n",
      ">66, d1[1.153] d2[0.028] g[11.371]\n",
      ">67, d1[0.131] d2[0.899] g[10.468]\n",
      ">68, d1[0.631] d2[0.251] g[10.272]\n",
      ">69, d1[0.115] d2[0.289] g[10.241]\n",
      ">70, d1[0.166] d2[0.170] g[9.342]\n",
      ">71, d1[0.103] d2[0.221] g[9.984]\n",
      ">72, d1[0.100] d2[0.081] g[9.515]\n",
      ">73, d1[0.104] d2[0.127] g[9.914]\n",
      ">74, d1[0.051] d2[0.035] g[9.055]\n",
      ">75, d1[0.033] d2[0.003] g[9.148]\n",
      ">76, d1[0.019] d2[0.097] g[9.196]\n",
      ">77, d1[0.023] d2[0.063] g[9.260]\n",
      ">78, d1[0.038] d2[0.013] g[9.594]\n",
      ">79, d1[0.026] d2[0.049] g[9.065]\n",
      ">80, d1[0.024] d2[0.171] g[9.288]\n",
      ">81, d1[0.116] d2[0.008] g[9.879]\n",
      ">82, d1[0.208] d2[0.007] g[9.288]\n",
      ">83, d1[0.003] d2[0.053] g[8.925]\n",
      ">84, d1[0.003] d2[0.017] g[9.077]\n",
      ">85, d1[0.010] d2[0.007] g[8.965]\n",
      ">86, d1[0.003] d2[0.025] g[8.946]\n",
      ">87, d1[0.002] d2[0.011] g[9.150]\n",
      ">88, d1[0.002] d2[0.124] g[8.773]\n",
      ">89, d1[0.016] d2[0.010] g[8.790]\n",
      ">90, d1[0.010] d2[0.006] g[8.674]\n",
      ">91, d1[0.007] d2[0.024] g[8.921]\n",
      ">92, d1[0.005] d2[0.059] g[9.040]\n",
      ">93, d1[0.014] d2[0.003] g[8.766]\n",
      ">94, d1[0.019] d2[0.003] g[9.180]\n",
      ">95, d1[0.008] d2[0.006] g[8.713]\n",
      ">96, d1[0.008] d2[0.001] g[9.376]\n",
      ">97, d1[0.005] d2[0.001] g[12.329]\n",
      ">98, d1[0.005] d2[0.005] g[10.527]\n",
      ">99, d1[0.003] d2[0.002] g[10.424]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">100, d1[0.003] d2[0.014] g[10.776]\n",
      ">Saved: images/plot_000100.png and models/model_000100.h5\n",
      ">101, d1[0.002] d2[0.006] g[8.737]\n",
      ">102, d1[0.003] d2[0.002] g[8.676]\n",
      ">103, d1[0.004] d2[0.002] g[8.704]\n",
      ">104, d1[0.003] d2[0.773] g[15.219]\n",
      ">105, d1[3.755] d2[0.000] g[18.828]\n",
      ">106, d1[0.211] d2[0.000] g[16.894]\n",
      ">107, d1[0.009] d2[0.014] g[13.135]\n",
      ">108, d1[0.002] d2[0.683] g[18.418]\n",
      ">109, d1[4.876] d2[0.277] g[10.046]\n",
      ">110, d1[0.004] d2[0.156] g[10.229]\n",
      ">111, d1[0.028] d2[0.102] g[9.599]\n",
      ">112, d1[0.059] d2[0.198] g[9.567]\n",
      ">113, d1[0.137] d2[0.074] g[9.819]\n",
      ">114, d1[0.075] d2[0.023] g[8.984]\n",
      ">115, d1[0.015] d2[0.394] g[9.727]\n",
      ">116, d1[0.176] d2[0.002] g[13.322]\n",
      ">117, d1[0.295] d2[0.082] g[9.775]\n",
      ">118, d1[1.216] d2[0.775] g[8.795]\n",
      ">119, d1[0.005] d2[0.198] g[9.360]\n",
      ">120, d1[0.025] d2[0.112] g[9.429]\n",
      ">121, d1[0.128] d2[0.047] g[9.802]\n",
      ">122, d1[0.011] d2[0.041] g[8.920]\n",
      ">123, d1[0.013] d2[0.011] g[8.831]\n",
      ">124, d1[0.008] d2[0.034] g[8.547]\n",
      ">125, d1[0.006] d2[0.042] g[8.320]\n",
      ">126, d1[0.008] d2[0.024] g[8.524]\n",
      ">127, d1[0.010] d2[0.003] g[8.596]\n",
      ">128, d1[0.006] d2[0.007] g[8.698]\n",
      ">129, d1[0.006] d2[0.013] g[8.208]\n",
      ">130, d1[0.005] d2[0.036] g[8.508]\n",
      ">131, d1[0.005] d2[0.001] g[8.333]\n",
      ">132, d1[0.007] d2[0.002] g[8.352]\n",
      ">133, d1[0.005] d2[0.003] g[8.096]\n",
      ">134, d1[0.004] d2[0.013] g[8.373]\n",
      ">135, d1[0.004] d2[0.001] g[8.247]\n",
      ">136, d1[0.003] d2[0.004] g[8.314]\n",
      ">137, d1[0.003] d2[0.002] g[8.347]\n",
      ">138, d1[0.003] d2[0.002] g[8.340]\n",
      ">139, d1[0.002] d2[0.010] g[8.541]\n",
      ">140, d1[0.004] d2[0.001] g[8.451]\n",
      ">141, d1[0.002] d2[0.008] g[8.189]\n",
      ">142, d1[0.002] d2[0.001] g[7.980]\n",
      ">143, d1[0.002] d2[0.002] g[7.993]\n",
      ">144, d1[0.003] d2[0.001] g[8.438]\n",
      ">145, d1[0.002] d2[0.023] g[8.385]\n",
      ">146, d1[0.003] d2[0.011] g[8.228]\n",
      ">147, d1[0.004] d2[0.000] g[8.315]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-dc68cfff484d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp2p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-9118a04e2432>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(discriminator, generator, gan, epochs, samplesPerEpoch)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# Update generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mgeneratorLoss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrealX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrealLabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealY\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# summarize performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1076\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m           \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1078\u001b[1;33m           standalone=True)\n\u001b[0m\u001b[0;32m   1079\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[0;32m   1080\u001b[0m                  outputs['metrics'])\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics, standalone)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m   \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1017\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_training_eval_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1019\u001b[1;33m       \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;31m# Reset metrics on all the distributed (cloned) models.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \"\"\"\n\u001b[1;32m--> 212\u001b[1;33m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   3321\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3322\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3323\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3324\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3325\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m    819\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[1;32m--> 821\u001b[1;33m           self.handle, value_tensor, name=name)\n\u001b[0m\u001b[0;32m    822\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op\u001b[1;34m(resource, value, name)\u001b[0m\n\u001b[0;32m    140\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m    141\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"AssignVariableOp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         tld.op_callbacks, resource, value)\n\u001b[0m\u001b[0;32m    143\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(d,g,p2p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-heading",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
