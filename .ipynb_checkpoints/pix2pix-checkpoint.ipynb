{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "noticed-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alternative-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder block based on the original paper\n",
    "def define_encoder_block(layer, filtersNo, batchnorm=True):\n",
    "    \n",
    "    # init weights from a Gaussian distribution with mean 0 and standard deviation 0.02\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    # in the original paper, all convolution kernels are (4,4), with stride 2. Stride for decoder means downsampling.\n",
    "    x = Conv2D(filtersNo, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer)\n",
    "    \n",
    "    # Conditional batch normalization (important for the first layer)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x, training=True)\n",
    "        \n",
    "    # All ReLUs in the encoder are leaky!\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "previous-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the decoder block based on the original paper\n",
    "def decoder_block(layer, skip, filtersNo, dropout=True, batch=True):\n",
    "    \n",
    "    # init weights from a Gaussian distribution with mean 0 and standard deviation 0.02\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    # in the original paper, all convolution kernels are (4,4), with stride 2. Stride for decoder means upsampling.\n",
    "    x = Conv2DTranspose(filtersNo, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer)\n",
    "\n",
    "    # All layers in the original paper have batch normalization, although we set an if statement just to play with the model\n",
    "    if batch:\n",
    "        x = BatchNormalization()(x, training=True)\n",
    "        \n",
    "    # Some decoder layers don't have dropout\n",
    "    if dropout:\n",
    "        x = Dropout(0.5)(x, training=True)\n",
    "        \n",
    "    # Merge with skip connection\n",
    "    x = Concatenate()([x, skip])\n",
    "    \n",
    "    # All ReLUs in the decoder are not leaky!\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gothic-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator based on encoder/decoder\n",
    "def define_generator():\n",
    "    \n",
    "    # init weights from a Gaussian distribution with mean 0 and standard deviation 0.02\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    # image input\n",
    "    inputImage = Input(shape=(128,128,1))\n",
    "    \n",
    "    ###### Encoder\n",
    "    \n",
    "    # C64, input (128,128,1), output (64,64,64)\n",
    "    encoderLayer1 = define_encoder_block(inputImage, 64, batchnorm=False)\n",
    "    \n",
    "    #C128, input (64,64,64), output (32,32,128)\n",
    "    encoderLayer2 = define_encoder_block(encoderLayer1, 128)\n",
    "    \n",
    "    #C256, input (32,32,128), output (16,16,256)\n",
    "    encoderLayer3 = define_encoder_block(encoderLayer2, 256)\n",
    "    \n",
    "    #C512, input (16,16,256), output (8,8,512)\n",
    "    encoderLayer4 = define_encoder_block(encoderLayer3, 512)\n",
    "    \n",
    "    #C512, input (8,8,512), output (4,4,512)\n",
    "    encoderLayer5 = define_encoder_block(encoderLayer4, 512)\n",
    "    \n",
    "    #C512, input (4,4,512), output (2,2,512)\n",
    "    encoderLayer6 = define_encoder_block(encoderLayer5, 512)\n",
    "    \n",
    "    ###### Bottleneck layer, will have an input of (2,2,512) and an output of (1,1,512)\n",
    "    bottleneck = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(encoderLayer6)\n",
    "    bottleneck = Activation('relu')(bottleneck)\n",
    "    \n",
    "    ###### Decoder, with skip connection\n",
    "    \n",
    "    #CD512\n",
    "    decoderLayer1 = decoder_block(bottleneck, encoderLayer6, 512)\n",
    "    \n",
    "    #CD512\n",
    "    decoderLayer2 = decoder_block(decoderLayer1, encoderLayer5, 512)\n",
    "    \n",
    "    #C512\n",
    "    decoderLayer3 = decoder_block(decoderLayer2, encoderLayer4, 512, dropout=False)\n",
    "    \n",
    "    #C256\n",
    "    decoderLayer4 = decoder_block(decoderLayer3, encoderLayer3, 256, dropout=False)\n",
    "    \n",
    "    #C128\n",
    "    decoderLayer5 = decoder_block(decoderLayer4, encoderLayer2, 128, dropout=False)\n",
    "    \n",
    "    #C64\n",
    "    decoderLayer6 = decoder_block(decoderLayer5, encoderLayer1, 64, dropout=False)\n",
    "    \n",
    "    # Output with tanh function, as mentioned in the original paper. Output will be (128x128x3)\n",
    "    g = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(decoderLayer6)\n",
    "    outputImage = Activation('tanh')(g)\n",
    "    \n",
    "    # Define model\n",
    "    model = Model(inputImage, outputImage)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "printable-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 70x70 discriminator as in the original paper\n",
    "def define_discriminator():\n",
    "    \n",
    "    # init weights from a Gaussian distribution with mean 0 and standard deviation 0.02\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    # source image input\n",
    "    source = Input(shape=(128,128,1))\n",
    "    \n",
    "    # target image input\n",
    "    target = Input(shape=(128,128,3))\n",
    "    \n",
    "    # concatenate images channel-wise\n",
    "    merged = Concatenate()([source, target])\n",
    "    \n",
    "    # C64\n",
    "    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    # C128\n",
    "    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    # C256\n",
    "    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    # C512\n",
    "    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    # patch output\n",
    "    d = Conv2D(1, (8,8), strides=(8,8), padding='same', kernel_initializer=init)(d)\n",
    "    patch_out = Activation('sigmoid')(d)\n",
    "    patch_out = Flatten()(patch_out)\n",
    "    \n",
    "    # define model\n",
    "    model = Model([source, target], patch_out)\n",
    "    \n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "loving-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pix2Pix GAN\n",
    "def pix2pix(generator, discriminator):\n",
    "    \n",
    "    # make weights in the discriminator not trainable\n",
    "    for layer in discriminator.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "            \n",
    "    # define the source image\n",
    "    source = Input(shape=(128,128,1))\n",
    "    \n",
    "    # connect the source image to the generator input\n",
    "    genOut = generator(source)\n",
    "    \n",
    "    # connect the source input and generator output to the discriminator input\n",
    "    disOut = discriminator([source, genOut])\n",
    "    \n",
    "    # src image as input, generated image and classification output\n",
    "    model = Model(source, [disOut, genOut])\n",
    "    \n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "previous-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFromDataset(trainX, trainY, samples):\n",
    "    \n",
    "    # Choose random images from both input and output\n",
    "    no = randint(0, trainX.shape[0], samples)\n",
    "    gx, gy = trainX[no], trainY[no]\n",
    "    \n",
    "    # Set y-labels to 1, as these images are from dataset\n",
    "    y = ones((samples, 1))\n",
    "    return [gx, gy], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fluid-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFromGenerator(generator, samples):\n",
    "    # Generate fake instance\n",
    "    x = generator.predict(samples)\n",
    "    \n",
    "    # Labels will be zero because they come from the generator\n",
    "    y = zeros((len(x), 1))\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "awful-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_performance(step, g_model, trainX, trainY, n_samples=3):\n",
    "    # select a sample of input images\n",
    "    [X_realA, X_realB], _ = generateFromDataset(trainX, trainY, n_samples)\n",
    "    \n",
    "    # generate a batch of fake samples\n",
    "    X_fakeB, _ = generateFromGenerator(g_model, X_realA)\n",
    "    \n",
    "    # scale all pixels from [-1,1] to [0,1]\n",
    "    #X_realA = (X_realA + 1) / 2.0\n",
    "    #X_realB = (X_realB + 1) / 2.0\n",
    "    #X_fakeB = (X_fakeB + 1) / 2.0\n",
    "    \n",
    "    # plot real source images\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(3, n_samples, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_realA[i])\n",
    "    # plot generated target image\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(3, n_samples, 1 + n_samples + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_fakeB[i])\n",
    "    # plot real target image\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_realB[i])\n",
    "    # save plot to file\n",
    "    filename1 = 'images/plot_%06d.png' % (step+1)\n",
    "    pyplot.savefig(filename1)\n",
    "    pyplot.close()\n",
    "    # save the generator model\n",
    "    filename2 = 'models/model_%06d.h5' % (step+1)\n",
    "    g_model.save(filename2)\n",
    "    print('>Saved: %s and %s' % (filename1, filename2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "musical-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(discriminator, generator, gan, epochs=100000, samplesPerEpoch=250):\n",
    "    \n",
    "    # Load the data\n",
    "    bwData = load(\"Flickr8kblackandwhite1dim.npy\")\n",
    "    colorData = load(\"flickr8k_shuffled.npy\")\n",
    "    y = numpy.ones((8091,1))\n",
    "    \n",
    "    BWSplit = numpy.array_split(bwData, 2)\n",
    "    colorSplit = numpy.array_split(colorData, 2)\n",
    "    \n",
    "    # manually enumerate epochs\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Generate real samples\n",
    "        [realX, realY], realLabel = generateFromDataset(BWSplit[0], colorSplit[0], samplesPerEpoch)\n",
    "        \n",
    "        # Generate fake samples\n",
    "        fakeY, fakeLabel = generateFromGenerator(generator, realX)\n",
    "        \n",
    "        # Update discriminator on real samples\n",
    "        realLoss = discriminator.train_on_batch([realX, realY], realLabel)\n",
    "        \n",
    "        # Update discriminator on fake samples\n",
    "        fakeLoss = discriminator.train_on_batch([realX, fakeY], fakeLabel)\n",
    "        \n",
    "        # Update generator\n",
    "        generatorLoss, _, _ = gan.train_on_batch(realX, [realLabel, realY])\n",
    "        \n",
    "        # summarize performance\n",
    "        print('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, realLoss, fakeLoss, generatorLoss))\n",
    "        if (i+1)%100 == 0:\n",
    "            summarize_performance(i, generator, BWSplit[0], colorSplit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "boxed-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = define_discriminator()\n",
    "g = define_generator()\n",
    "p2p = pix2pix(g,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "coated-russian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, d1[0.540] d2[4.168] g[45.360]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Saved: plot_000001.png and model_000001.h5\n",
      ">2, d1[0.011] d2[4.578] g[43.647]\n",
      ">3, d1[0.528] d2[3.485] g[43.986]\n",
      ">4, d1[1.522] d2[3.047] g[40.490]\n",
      ">5, d1[0.546] d2[2.287] g[40.205]\n",
      ">6, d1[0.869] d2[1.093] g[40.966]\n",
      ">7, d1[0.274] d2[0.365] g[38.466]\n",
      ">8, d1[0.216] d2[1.819] g[39.140]\n",
      ">9, d1[1.780] d2[1.860] g[36.276]\n",
      ">10, d1[0.030] d2[0.111] g[34.029]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">11, d1[0.183] d2[1.566] g[35.109]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-dc68cfff484d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp2p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-4ddbd6261329>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(discriminator, generator, gan, epochs, samplesPerEpoch)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'>%d, d1[%.3f] d2[%.3f] g[%.3f]'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealLoss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfakeLoss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeneratorLoss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msamplesPerEpoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2500\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0msummarize_performance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBWSplit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolorSplit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-0dd7ba8c9917>\u001b[0m in \u001b[0;36msummarize_performance\u001b[1;34m(step, g_model, trainX, trainY, n_samples)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# save the generator model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mfilename2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'model_%06d.h5'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mg_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'>Saved: %s and %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfilename1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \"\"\"\n\u001b[0;32m   1007\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[1;32m-> 1008\u001b[1;33m                     signatures, options)\n\u001b[0m\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m    110\u001b[0m           'or using `save_weights`.')\n\u001b[0;32m    111\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[1;32m--> 112\u001b[1;33m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[0;32m    113\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_weights'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mmodel_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_weights_group\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;31m# TODO(b/128683857): Add integration tests between tf.keras and external\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[1;34m(f, layers)\u001b[0m\n\u001b[0;32m    634\u001b[0m         \u001b[0mparam_dset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m         \u001b[0mparam_dset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\h5py\\_hl\\dataset.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, args, val)\u001b[0m\n\u001b[0;32m    706\u001b[0m         \u001b[0mmspace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmshape_pad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh5s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNLIMITED\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmshape_pad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfspace\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mselection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread_direct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_sel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest_sel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(d,g,p2p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-genius",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
